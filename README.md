# speech-emotion-recognition

Audio Emotion Recognition Dataset: Utilized a dataset containing 2800 audio samples across seven emotions, including fear, angry, disgust, neutral, sad, ps, and happy, with associated paths and labels.

Feature Extraction and Visualization: Employed librosa to extract MFCC features and visualize audio characteristics through waveplots and spectrograms, providing insights into the emotional content of the audio.

Deep Learning Model: Developed a deep learning model using LSTM layers for emotion classification, achieving training and validation accuracies over 50 epochs and visualized the training process.

SVM Emotion Classification: Implemented an SVM model for emotion classification using extracted MFCC features, achieving high accuracy and visualizing results through a confusion matrix.

Predictive Functionality: Created a predictive function for emotion recognition in new audio files using both the trained deep learning model and the SVM model, showcasing the versatility of the implemented approach.



